{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"white.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.concat([data2.iloc[:,0:11], pd.get_dummies(data2[\"y\"])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimini(x, size, seed = 0): # 미니배치 구현\n",
    "    np.random.seed(seed)\n",
    "    m = x.shape[0]\n",
    "    mini = []\n",
    "    per = list(np.random.permutation(m))\n",
    "    shuffle_x = x.iloc[per, :]\n",
    "    num = math.floor(m/size)\n",
    "    for k in range(num):\n",
    "        mini.append(shuffle_x.iloc[k*size:(k+1)*size, : ])\n",
    "    if m % size != 0:\n",
    "        mini.append(shuffle_x.iloc[num*size:m, :])\n",
    "    return mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1번\n",
    "sigmoid 함수 이용, GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 69.93737009951943\n",
      "epoch10 12.007026622169892\n",
      "epoch20 6.8725834896689975\n",
      "epoch30 4.8468804673144685\n",
      "epoch40 3.7609592174228865\n",
      "epoch50 3.0623210856789047\n",
      "epoch60 2.6125997210803784\n",
      "epoch70 2.271466562622472\n",
      "epoch80 2.012270667051014\n",
      "epoch90 1.8129157800423468\n",
      "epoch100 1.6458870015646279\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "dim = 11\n",
    "learn_rate = 0.001\n",
    "train_epoch = 101\n",
    "batch_size = 128\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 11])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 7])\n",
    "\n",
    "w0 = tf.Variable(tf.zeros(shape = [dim, 256]))\n",
    "w1 = tf.Variable(tf.zeros(shape = [256, 128]))\n",
    "w2 = tf.Variable(tf.zeros(shape = [128, 7]))\n",
    "\n",
    "b0 = tf.Variable(tf.zeros(shape = [256]))\n",
    "b1 = tf.Variable(tf.zeros(shape = [128]))\n",
    "b2 = tf.Variable(tf.zeros(shape = [7]))\n",
    "\n",
    "def classifier(x):\n",
    "    c1 = tf.nn.relu(tf.add(tf.matmul(x, w0), b0))\n",
    "    c2 = tf.nn.relu(tf.add(tf.matmul(c1, w1), b1))\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(c2, w2), b2))\n",
    "\n",
    "y = classifier(X)\n",
    "cost = -tf.reduce_sum(Y*tf.log(y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(train_epoch):\n",
    "    avg_cost = 0\n",
    "    a = minimini(data2, batch_size, seed = i)\n",
    "    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))\n",
    "    for step in range(math.floor(data2.shape[0]/batch_size)):\n",
    "            temp = a[gr[step]]\n",
    "            xs = temp.iloc[:, 0:11]\n",
    "            ys = temp.iloc[:, 11:]\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys})\n",
    "            avg_cost += c/math.floor(data2.shape[0]/batch_size)\n",
    "    if i % 10  == 0:\n",
    "        print(\"epoch\"+str(i), avg_cost)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2번\n",
    "\n",
    "Layer 확대와 Node 수 변경, sigmoid 함수 이용, Adamoptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 87.5692028246428\n",
      "epoch10 67.00544457686576\n",
      "epoch20 51.70519858912419\n",
      "epoch30 40.350440778230364\n",
      "epoch40 31.870448765001793\n",
      "epoch50 25.459972682752106\n",
      "epoch60 20.558636012830227\n",
      "epoch70 16.749345428065247\n",
      "epoch80 13.752781943271033\n",
      "epoch90 11.368091156608184\n",
      "epoch100 9.448351508692694\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "dim = 11\n",
    "learn_rate = 0.001\n",
    "train_epoch = 101\n",
    "batch_size = 128\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 11])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 7])\n",
    "\n",
    "w0 = tf.Variable(tf.zeros(shape = [dim, 512]))\n",
    "w1 = tf.Variable(tf.zeros(shape = [512, 256]))\n",
    "w2 = tf.Variable(tf.zeros(shape = [256, 128]))\n",
    "w3 = tf.Variable(tf.zeros(shape = [128, 7]))\n",
    "\n",
    "b0 = tf.Variable(tf.zeros(shape = [512]))\n",
    "b1 = tf.Variable(tf.zeros(shape = [256]))\n",
    "b2 = tf.Variable(tf.zeros(shape = [128]))\n",
    "b3 = tf.Variable(tf.zeros(shape = [7]))\n",
    "\n",
    "def classifier(x):\n",
    "    c1 = tf.nn.relu(tf.add(tf.matmul(x, w0), b0))\n",
    "    c2 = tf.nn.relu(tf.add(tf.matmul(c1, w1), b1))\n",
    "    c3 = tf.nn.relu(tf.add(tf.matmul(c2, w2), b2))\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(c3, w3), b3))\n",
    "\n",
    "y = classifier(X)\n",
    "cost = -tf.reduce_sum(Y*tf.log(y))\n",
    "optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(train_epoch):\n",
    "    avg_cost = 0\n",
    "    a = minimini(data2, batch_size, seed = i)\n",
    "    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))\n",
    "    for step in range(math.floor(data2.shape[0]/batch_size)):\n",
    "            temp = a[gr[step]]\n",
    "            xs = temp.iloc[:, 0:11]\n",
    "            ys = temp.iloc[:, 11:]\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys})\n",
    "            avg_cost += c/math.floor(data2.shape[0]/batch_size)\n",
    "    if i % 10  == 0:\n",
    "        print(\"epoch\"+str(i), avg_cost)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3번\n",
    "Weight 초기화하는 Xavier 알고리즘, Layer 확대와 Node 수 변경, Adamoptimizer, relu 함수 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 204.29310648064862\n",
      "epoch10 -920.9188874897204\n",
      "epoch20 -1175.0350422106292\n",
      "epoch30 -1306.5669973273025\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "dim = 11\n",
    "learn_rate = 0.001\n",
    "train_epoch = 31\n",
    "batch_size = 128\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 11])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 7])\n",
    "\n",
    "def xavier(size):\n",
    "    std = 1. / tf.sqrt(size[0]/2.)\n",
    "    return tf.random_normal(shape = size, stddev = std)\n",
    "\n",
    "w0 = tf.Variable(xavier(size = [dim, 512]))\n",
    "w1 = tf.Variable(xavier(size = [512, 256]))\n",
    "w2 = tf.Variable(xavier(size = [256, 128]))\n",
    "w3 = tf.Variable(xavier(size = [128, 7]))\n",
    "\n",
    "b0 = tf.Variable(tf.zeros(shape = [512]))\n",
    "b1 = tf.Variable(tf.zeros(shape = [256]))\n",
    "b2 = tf.Variable(tf.zeros(shape = [128]))\n",
    "b3 = tf.Variable(tf.zeros(shape = [7]))\n",
    "\n",
    "def classifier(x):\n",
    "    c1 = tf.nn.relu(tf.add(tf.matmul(x, w0), b0))\n",
    "    c2 = tf.nn.relu(tf.add(tf.matmul(c1, w1), b1))\n",
    "    c3 = tf.nn.relu(tf.add(tf.matmul(c2, w2), b2))\n",
    "    return tf.nn.relu(tf.add(tf.matmul(c3, w3), b3))\n",
    "\n",
    "y = classifier(X)\n",
    "cost = -tf.reduce_sum(Y*tf.log(y + 1e-6))\n",
    "optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(train_epoch):\n",
    "    avg_cost = 0\n",
    "    a = minimini(data2, batch_size, seed = i)\n",
    "    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))\n",
    "    for step in range(math.floor(data2.shape[0]/batch_size)):\n",
    "            temp = a[gr[step]]\n",
    "            xs = temp.iloc[:, 0:11]\n",
    "            ys = temp.iloc[:, 11:]\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys})\n",
    "            avg_cost += c/math.floor(data2.shape[0]/batch_size)\n",
    "    if i % 10  == 0:\n",
    "        print(\"epoch\"+str(i), avg_cost)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4번\n",
    "Dropout, sigmoid 함수 이용, GradientDescentOptimizer, Layer 확대와 Node 수 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 69.93737009951943\n",
      "epoch10 12.007026622169892\n",
      "epoch20 6.8725834896689975\n",
      "epoch30 4.8468804673144685\n",
      "epoch40 3.7609592174228865\n",
      "epoch50 3.0623210856789047\n",
      "epoch60 2.6125997210803784\n",
      "epoch70 2.271466562622472\n",
      "epoch80 2.012270667051014\n",
      "epoch90 1.8129157800423468\n",
      "epoch100 1.6458870015646279\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "dim = 11\n",
    "learn_rate = 0.001\n",
    "train_epoch = 101\n",
    "batch_size = 128\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 11])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 7])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "w0 = tf.Variable(tf.zeros(shape = [dim, 512]))\n",
    "w1 = tf.Variable(tf.zeros(shape = [512, 256]))\n",
    "w2 = tf.Variable(tf.zeros(shape = [256, 128]))\n",
    "w3 = tf.Variable(tf.zeros(shape = [128, 7]))\n",
    "\n",
    "b0 = tf.Variable(tf.zeros(shape = [512]))\n",
    "b1 = tf.Variable(tf.zeros(shape = [256]))\n",
    "b2 = tf.Variable(tf.zeros(shape = [128]))\n",
    "b3 = tf.Variable(tf.zeros(shape = [7]))\n",
    "\n",
    "\n",
    "def classifier(x, keep_prob):\n",
    "    c1 = tf.nn.relu(tf.add(tf.matmul(x, w0), b0))\n",
    "    c2 = tf.nn.relu(tf.add(tf.matmul(c1, w1), b1))\n",
    "    c3 = tf.nn.relu(tf.add(tf.matmul(c2, w2), b2))\n",
    "    c_drop = tf.nn.dropout(c3, keep_prob)\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(c_drop, w3), b3))\n",
    "\n",
    "y = classifier(X, keep_prob)\n",
    "cost = -tf.reduce_sum(Y*tf.log(y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(train_epoch):\n",
    "    avg_cost = 0\n",
    "    a = minimini(data2, batch_size, seed = i)\n",
    "    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))\n",
    "    for step in range(math.floor(data2.shape[0]/batch_size)):\n",
    "            temp = a[gr[step]]\n",
    "            xs = temp.iloc[:, 0:11]\n",
    "            ys = temp.iloc[:, 11:]\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys, keep_prob: 0.7})\n",
    "            avg_cost += c/math.floor(data2.shape[0]/batch_size)\n",
    "    if i % 10  == 0:\n",
    "        print(\"epoch\"+str(i), avg_cost)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5번\n",
    "Dropout, relu 함수 이용, Layer 확대와 Node 수 변경, Adamoptimizer, Weight 초기화하는 Xavier 알고리즘 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 426.54243388928876\n",
      "epoch10 342.0379253186678\n",
      "epoch20 317.96462450529395\n",
      "epoch30 294.3061186137951\n",
      "epoch40 272.4664772435238\n",
      "epoch50 252.35262419048112\n",
      "epoch60 233.80257817318565\n",
      "epoch70 216.48261301141034\n",
      "epoch80 200.14309290835732\n",
      "epoch90 184.57526076467417\n",
      "epoch100 169.67199385793583\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "dim = 11\n",
    "learn_rate = 0.01\n",
    "train_epoch = 101\n",
    "batch_size = 128\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 11])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 7])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def xavier(size):\n",
    "    std = 1. / tf.sqrt(size[0]/2.)\n",
    "    return tf.random_normal(shape = size, stddev = std)\n",
    "\n",
    "w0 = tf.Variable(tf.zeros(shape = [dim, 256]))\n",
    "w1 = tf.Variable(tf.zeros(shape = [256, 128]))\n",
    "w2 = tf.Variable(tf.zeros(shape = [128, 7]))\n",
    "\n",
    "b0 = tf.Variable(tf.zeros(shape = [256]))\n",
    "b1 = tf.Variable(tf.zeros(shape = [128]))\n",
    "b2 = tf.Variable(tf.zeros(shape = [7]))\n",
    "\n",
    "def classifier(x, keep_prob):\n",
    "    c1 = tf.nn.elu(tf.add(tf.matmul(x, w0), b0))\n",
    "    c2 = tf.nn.elu(tf.add(tf.matmul(c1, w1), b1))\n",
    "    c_drop = tf.nn.dropout(c2, keep_prob)\n",
    "    return tf.nn.elu(tf.add(tf.matmul(c_drop, w2), b2))\n",
    "\n",
    "y = classifier(X, keep_prob)\n",
    "\n",
    "cost = -tf.reduce_sum(Y*tf.log(y + 1e-5))\n",
    "optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(train_epoch):\n",
    "    avg_cost = 0\n",
    "    a = minimini(data2, batch_size, seed = i)\n",
    "    gr = list(np.random.permutation(math.floor(data2.shape[0]/batch_size)))\n",
    "    for step in range(math.floor(data2.shape[0]/batch_size)):\n",
    "            temp = a[gr[step]]\n",
    "            xs = temp.iloc[:, 0:11]\n",
    "            ys = temp.iloc[:, 11:]\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X: xs, Y: ys, keep_prob: 0.1})\n",
    "            avg_cost += c/math.floor(data2.shape[0]/batch_size)\n",
    "    if i % 10  == 0:\n",
    "        print(\"epoch\"+str(i), avg_cost)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론\n",
    "\n",
    "1. 자비어 친구를 적용하면 어느정도 처음 로스가 줄어든다? -> 실험이 좀 더 필요\n",
    "2. relu로 너무 많이 쌓으면 안좋은거 같기도 하다 가끔 nan 나옴 ㅠㅠ\n",
    "3. 미니배치의 필요성... 안그러면 학습이 너무 오래걸린다\n",
    "4. relu dying problem이 너무 많이 나서 tf.clip_by_value(y_conv,1e-10,1.0) 이런식으로 최소 최대 값을 잡았다 3번과 5번 비교해보길 -> relu에서 elu로 바꿔버림\n",
    "5. Adam은 GD과 수렴속도가 달라서 약간 learning rate의 조절이 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
