{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# library import, data load\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"mnist/data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 88.473696450\n",
      "Epoch: 0002 cost= 31.555418206\n",
      "Epoch: 0003 cost= 20.070919518\n",
      "Epoch: 0004 cost= 14.866313527\n",
      "Epoch: 0005 cost= 11.097318321\n",
      "Epoch: 0006 cost= 8.410567877\n",
      "Epoch: 0007 cost= 6.606693545\n",
      "Epoch: 0008 cost= 4.920761621\n",
      "Epoch: 0009 cost= 4.408934270\n",
      "Epoch: 0010 cost= 3.420069583\n",
      "Epoch: 0011 cost= 2.599074087\n",
      "Epoch: 0012 cost= 2.972526207\n",
      "Epoch: 0013 cost= 2.509986854\n",
      "Epoch: 0014 cost= 2.816935666\n",
      "Epoch: 0015 cost= 3.322123467\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9776\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 256\n",
    "dim = 784\n",
    "\n",
    "### 신경망 모델 구성\n",
    "\n",
    "def xavier(size):\n",
    "    std = 1. / tf.sqrt(size[0]/2.)\n",
    "    return tf.random_normal(shape = size, stddev = std)\n",
    "# 자비어 이니셜라이즈 함수 구현\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "w0 = tf.Variable(xavier([dim, 256]))\n",
    "w1 = tf.Variable(xavier([256, 128]))\n",
    "w2 = tf.Variable(xavier([128, 64]))\n",
    "w3 = tf.Variable(xavier([64, 10]))\n",
    "\n",
    "b0 = tf.Variable(tf.zeros(shape = [256]))\n",
    "b1 = tf.Variable(tf.zeros(shape = [128]))\n",
    "b2 = tf.Variable(tf.zeros(shape = [64]))\n",
    "b3 = tf.Variable(tf.zeros(shape = [10]))\n",
    "# 네트워크 층 친구들\n",
    "\n",
    "\n",
    "def classifier(x):\n",
    "    c1 = tf.nn.relu(tf.matmul(x, w0) + b0)\n",
    "    c2 = tf.nn.relu(tf.matmul(c1, w1) + b1)\n",
    "    c3 = tf.nn.relu(tf.matmul(c2, w2) + b2)\n",
    "    return tf.nn.softmax(tf.matmul(c3, w3) + b3)\n",
    "# 렐루렐루렐루 \n",
    "\n",
    "hypo = classifier(x)\n",
    "cost = -tf.reduce_sum(y_*tf.log(hypo)) # cross entropy sum\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "### 신경망 모델 학습\n",
    "# Launch the graph,\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "        # Fit the line.\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += c/total_batch\n",
    "            \n",
    "            # Display \n",
    "        print (\"Epoch:\", '%04d' %(epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    ### 결과 확인\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypo, 1), tf.argmax(y_, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 진행한 작업들\n",
    "\n",
    "1. 레이어 수를 늘림\n",
    "2. 자비어 초기값을 줌\n",
    "3. 층을 늘림\n",
    "4. 여러 실험 결과 성능은 약 97 ~98 정도 인듯\n",
    "5. 위에 결과는 reduce sum을 사용해서 커보이는 것임 평균을 내면 엄청 줄어든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
